# Excuse Machine

A lightweight, local **humorous excuse generator**.  
- Frontend: HTML5, CSS3, javascript
- Backend: FastAPI, Python 3.11
- Model: Ollama + phi3:mini (local LLM)
- Container: Docker Compose
Users type any situation (e.g., “missed a deadline”) and get a funny, sarcastic or absurd excuse generated by a local LLM.

---

## Overview

### **Frontend**
- Simple, accessible web page (`frontend/`)
- Contains:
  - Input box for user text  
  - Style selector (sarcastic / absurd / creator / etc.)  
  - Output box  
  - Two buttons: **Generate** and **Copy**

### **Backend (FastAPI)**
- Located in `backend/`
- Handles API requests from the frontend:
  1. Receives text + style from user  
  2. Cleans and formats input (`utils/text_cleaner.py`)  
  3. Selects prompt template & parameters (`pipeline.py`)  
  4. Calls local model (`llm_client.py`) via HTTP  
  5. Returns the generated excuse as JSON  

### **Local Model (Ollama + phi3:mini)**
- Runs inside Docker (`ollama-appECM`)
- Backend communicates with it through `http://ollama-appECM:11434`
- Model generates short English excuses in different tones  
- Fully offline and privacy-safe.

---

## How to Run

### **Start Docker**
Make sure Docker is running, then from the project root:

```bash
docker compose up -d --build
```
- This will start:
  1. ollama-appECM → the model service
  2. excuse-backend → the FastAPI API
- Backend available at: http://localhost:3000
- Simply open the file: frontend/index.html

